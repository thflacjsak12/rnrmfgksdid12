{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(\"images/moon.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "result = detector.detect_faces(image)\n",
    "\n",
    "# Result is an array with all the bounding boxes detected. We know that for 'ivan.jpg' there is only one.\n",
    "bounding_box = result[0]['box']\n",
    "keypoints = result[0]['keypoints']\n",
    "\n",
    "cv2.rectangle(image,\n",
    "              (bounding_box[0], bounding_box[1]),\n",
    "              (bounding_box[0]+bounding_box[2], bounding_box[1] + bounding_box[3]),\n",
    "              (0,155,255),\n",
    "              2)\n",
    "\n",
    "cv2.circle(image,(keypoints['left_eye']), 2, (0,155,255), 2)\n",
    "cv2.circle(image,(keypoints['right_eye']), 2, (0,155,255), 2)\n",
    "cv2.circle(image,(keypoints['nose']), 2, (0,155,255), 2)\n",
    "cv2.circle(image,(keypoints['mouth_left']), 2, (0,155,255), 2)\n",
    "cv2.circle(image,(keypoints['mouth_right']), 2, (0,155,255), 2)\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "# cv2.imwrite(\"output_image.jpg\", image)\n",
    "\n",
    "# print(result)\n",
    "\n",
    "cv2.imshow('Face Detection Results', image)\n",
    "\n",
    "cv2.waitKey(0)     \n",
    "cv2.destroyAllWindows()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True: \n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # MTCNN 활용\n",
    "    result = detector.detect_faces(frame)\n",
    "    if result != []:\n",
    "        for person in result:\n",
    "            bounding_box = person['box']\n",
    "            keypoints = person['keypoints']\n",
    "    \n",
    "            cv2.rectangle(frame,\n",
    "                          (bounding_box[0], bounding_box[1]),\n",
    "                          (bounding_box[0]+bounding_box[2], bounding_box[1] + bounding_box[3]),\n",
    "                          (0,155,255),\n",
    "                          2)\n",
    "    \n",
    "            cv2.circle(frame,(keypoints['left_eye']), 2, (0,155,255), 2)\n",
    "            cv2.circle(frame,(keypoints['right_eye']), 2, (0,155,255), 2)\n",
    "            cv2.circle(frame,(keypoints['nose']), 2, (0,155,255), 2)\n",
    "            cv2.circle(frame,(keypoints['mouth_left']), 2, (0,155,255), 2)\n",
    "            cv2.circle(frame,(keypoints['mouth_right']), 2, (0,155,255), 2)\n",
    "    #display resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) == 13: #13 은 엔터키\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_classifier = cv2.CascadeClassifier('Haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_classifier.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30)\n",
    "    )\n",
    "\n",
    "    # 얼굴에 직사각형 그리기\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_classifier.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,0,255),2) \n",
    "\n",
    "    # 결과 출력하기\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 13: #13 은 엔터키\n",
    "        break\n",
    "        \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection import ObjectDetection\n",
    "import os\n",
    "\n",
    "execution_path = os.getcwd()\n",
    "# print(execution_path)\n",
    "\n",
    "# 모델 설정하기\n",
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath( os.path.join(execution_path , \"models/yolo.h5\"))\n",
    "\n",
    "# 모델 성능 설정하기\n",
    "detector.loadModel(detection_speed=\"normal\") #fast, faster, fastest, flash\n",
    "\n",
    "# 모델에 이미지 입력 및 출력 설정하기\n",
    "detections = detector.detectObjectsFromImage(\n",
    "    input_image=os.path.join(execution_path , \"images/street.jpg\"), \n",
    "    output_image_path=os.path.join(execution_path , \"image_out.jpg\"), \n",
    "    minimum_percentage_probability=50)\n",
    "\n",
    "# 실행결과 출력하기\n",
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
    "    print(\"--------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection import VideoObjectDetection\n",
    "import os\n",
    "\n",
    "execution_path = os.getcwd()\n",
    "\n",
    "detector = VideoObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath( os.path.join(execution_path , \"models/yolo.h5\"))\n",
    "detector.loadModel()\n",
    "\n",
    "video_path = detector.detectObjectsFromVideo(\n",
    "    input_file_path=os.path.join(execution_path, \"videos/seoul_01_0.mp4\"),\n",
    "    output_file_path=os.path.join(execution_path, \"video_out2\"),\n",
    "    frames_per_second=20, log_progress=True)\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection import VideoObjectDetection\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "execution_path = os.getcwd()\n",
    "\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "detector = VideoObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath( os.path.join(execution_path , \"models/yolo.h5\"))\n",
    "detector.loadModel()\n",
    "\n",
    "\n",
    "video_path = detector.detectObjectsFromVideo(\n",
    "                camera_input=camera,\n",
    "                output_file_path=os.path.join(execution_path, \"camera_detected_video\"),\n",
    "                frames_per_second=20, log_progress=True, minimum_percentage_probability=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
